{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5TfHM7KGZuu1",
        "0JuCQiJQZ3-Y"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "Nmgb9wu59VCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owLDtqDHaYsH"
      },
      "outputs": [],
      "source": [
        "from os import path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import pickle\n",
        "import pathlib\n",
        "import collections\n",
        "import urllib\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the dataset"
      ],
      "metadata": {
        "id": "5TfHM7KGZuu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_img_path = pathlib.Path('flickr8k') / 'Flicker8k_Dataset'"
      ],
      "metadata": {
        "id": "N_bXre-gL092"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/image_captioning.ipynb#scrollTo=kaNy_l7tGuAZ&line=1&uniqifier=1\n",
        "\n",
        "def flickr8k(path='flickr8k'):\n",
        "  path = pathlib.Path(path)\n",
        "  path = pathlib.Path(path)\n",
        "  dataset_path = path / 'Flicker8k_Dataset'\n",
        "\n",
        "  if not dataset_path.exists():\n",
        "    url = 'https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip'\n",
        "    file_path, _ = urllib.request.urlretrieve(url)\n",
        "    zip_ref = zipfile.ZipFile(file_path, 'r')\n",
        "    zip_ref.extractall(path)\n",
        "    zip_ref.close()\n",
        "\n",
        "    url = 'https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip'\n",
        "    file_path, _ = urllib.request.urlretrieve(url)\n",
        "    zip_ref = zipfile.ZipFile(file_path, 'r')\n",
        "    zip_ref.extractall(path)\n",
        "    zip_ref.close()\n",
        "    \n",
        "  captions = (path/\"Flickr8k.token.txt\").read_text().splitlines()\n",
        "  captions = (line.split('\\t') for line in captions)\n",
        "  captions = ((fname.split('#')[0], caption) for (fname, caption) in captions)\n",
        "\n",
        "  cap_dict = collections.defaultdict(list)\n",
        "  for fname, cap in captions:\n",
        "    cap_dict[fname].append(cap)\n",
        "\n",
        "  train_files = (path/'Flickr_8k.trainImages.txt').read_text().splitlines()\n",
        "  train_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in train_files]\n",
        "\n",
        "  dev_files = (path/'Flickr_8k.devImages.txt').read_text().splitlines()\n",
        "  dev_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in dev_files]\n",
        "\n",
        "  test_files = (path/'Flickr_8k.testImages.txt').read_text().splitlines()\n",
        "  test_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in test_files]\n",
        "\n",
        "  return train_captions, dev_captions, test_captions"
      ],
      "metadata": {
        "id": "gCcXQ78Lvh71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_raw, dev_raw, test_raw = flickr8k()"
      ],
      "metadata": {
        "id": "T-lZhY-tvpbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_captions = [cap for _, captions in train_raw for cap in captions]\n",
        "dev_captions = [cap for _, captions in dev_raw for cap in captions]\n",
        "test_captions = [cap for _, captions in test_raw for cap in captions]"
      ],
      "metadata": {
        "id": "rkapWqRc4fpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Features"
      ],
      "metadata": {
        "id": "xq4yXAMkB3Ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "rgZIta6zLBSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caption Embeddings"
      ],
      "metadata": {
        "id": "0JuCQiJQZ3-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "bBndiK7a9jX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('nreimers/MiniLM-L6-H384-uncased', device=device)"
      ],
      "metadata": {
        "id": "myxRiulDYQh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_embeddings = model.encode(train_captions, batch_size=256)\n",
        "dev_embeddings = model.encode(dev_captions, batch_size=256)\n",
        "test_embeddings = model.encode(test_captions, batch_size=256)"
      ],
      "metadata": {
        "id": "pehSSCl0-jGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image vectors"
      ],
      "metadata": {
        "id": "bnDBcSI4BswM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.io import read_image\n",
        "\n",
        "weights = torchvision.models.ResNet18_Weights.DEFAULT"
      ],
      "metadata": {
        "id": "VFnIYAOWCDG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare The Dataset"
      ],
      "metadata": {
        "id": "ZYG204ujtgWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "  def __init__(self, image_paths, transform=None, target_transform=None):\n",
        "    self.image_paths = image_paths\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_paths)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = self.image_paths[idx]\n",
        "    image = read_image(str(img_path))\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    return image"
      ],
      "metadata": {
        "id": "1XEutOOHNHVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_paths = [path for path, _ in train_raw]\n",
        "dev_img_paths = [path for path, _ in dev_raw]\n",
        "test_img_paths = [path for path, _ in test_raw]"
      ],
      "metadata": {
        "id": "Ikut8YSGMLj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = weights.transforms()"
      ],
      "metadata": {
        "id": "xWIyNbZSDtPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_train_ds = CustomImageDataset(train_img_paths, transform=transforms)\n",
        "img_dev_ds = CustomImageDataset(dev_img_paths, transform=transforms)\n",
        "img_test_ds = CustomImageDataset(test_img_paths, transform=transforms)"
      ],
      "metadata": {
        "id": "HB3Fup4gOLtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_train_dataloader = DataLoader(img_train_ds, batch_size=64, shuffle=False)\n",
        "img_dev_dataloader = DataLoader(img_dev_ds, batch_size=64, shuffle=False)\n",
        "img_test_dataloader = DataLoader(img_test_ds, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "3VrHSt-zO1zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract and save features"
      ],
      "metadata": {
        "id": "xLU-VanyvUQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_vectors(model, dataloader, device):\n",
        "  vectors = []\n",
        "\n",
        "  for imgs in dataloader:\n",
        "    hiddens = model(imgs.to(device))\n",
        "    cpu_hiddens = hiddens.cpu().detach().numpy().copy()\n",
        "    cpu_hiddens = np.squeeze(cpu_hiddens)\n",
        "\n",
        "    vectors.append(cpu_hiddens)\n",
        "\n",
        "  vectors = np.concatenate(vectors, axis=0)\n",
        "  return vectors\n",
        "\n",
        "def save_vectors(vectors, name, base_path):\n",
        "  vectors_path = pathlib.Path(base_path).joinpath(f'{name}_vectors.pkl')\n",
        "  with open(vectors_path, 'wb') as f:\n",
        "    pickle.dump(vectors, f)"
      ],
      "metadata": {
        "id": "njej7Be_vXqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.resnet18(weights)\n",
        "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
        "model = model.to(device)\n",
        "model = model.eval()"
      ],
      "metadata": {
        "id": "uCJUZXkjBsPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_vectors = extract_vectors(model, img_train_dataloader, device)\n",
        "dev_vectors = extract_vectors(model, img_dev_dataloader, device)\n",
        "test_vectors = extract_vectors(model, img_test_dataloader, device)"
      ],
      "metadata": {
        "id": "ADaQ-0pzOyq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive/MyDrive/collective_learning/px-multimodal-repr/binaries/flickr8k/'\n",
        "\n",
        "save_vectors(train_embeddings, 'train_text', base_path)\n",
        "save_vectors(dev_embeddings, 'dev_text', base_path)\n",
        "save_vectors(test_embeddings, 'test_text', base_path)\n",
        "save_vectors(train_vectors, 'train_img', base_path)\n",
        "save_vectors(dev_vectors, 'dev_img', base_path)\n",
        "save_vectors(test_vectors, 'test_img', base_path)"
      ],
      "metadata": {
        "id": "89RiXiwpT1Y9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}