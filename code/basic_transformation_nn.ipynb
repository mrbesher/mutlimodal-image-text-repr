{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iA9w3mVcejq0"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pathlib\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import trange\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Arguments\n",
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "NAwhbAJ857lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "Cxhg_SBNn9m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "nc2i47dnfjbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity_matrix(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Source: https://stackoverflow.com/a/50426321\n",
        "  \"\"\"\n",
        "  a = a / a.norm(dim=-1, keepdim=True)\n",
        "  b = b / b.norm(dim=-1, keepdim=True)\n",
        "  return a @ b.t()"
      ],
      "metadata": {
        "id": "dY3zGRRBQQ5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_negative_criterion(vectors_a: torch.Tensor, vectors_b: torch.Tensor, labels: torch.Tensor, loss_fn: torch.nn.TripletMarginLoss) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Calculates triplet loss to map representations `vectors_a` and `vectors_b` to the same vectors.\n",
        "  The hard negative sample is chosen based on the dot product between `vectors_a` and `vectors_b`.\n",
        "  The function returns the calculated loss value.\n",
        "\n",
        "  Parameters:\n",
        "  vectors_a (torch.Tensor): A tensor of shape (batch_size, embedding_size) representing the embeddings for the first set of vectors.\n",
        "  vectors_b (torch.Tensor): A tensor of shape (batch_size, embedding_size) representing the embeddings for the second set of vectors.\n",
        "  labels (torch.Tensor): A tensor of shape (batch_size) representing the label / class for each sample in `vectors_a` and `vectors_b`.\n",
        "  loss_fn ([torch.nn.TripletMarginLoss], optional): The triplet margin loss function. Defaults to `loss_fn` defined in the global scope.\n",
        "\n",
        "  Returns:\n",
        "  torch.Tensor: A tensor representing the calculated loss value.\n",
        "  \"\"\"\n",
        "  non_positive_msk = (labels.unsqueeze(0) != labels.unsqueeze(1))\n",
        "\n",
        "  # dot_product = torch.matmul(vectors_a, vectors_b.t())\n",
        "  negative_msk = torch.max(torch.rand(non_positive_msk.shape) * non_positive_msk, dim=1).indices\n",
        "  loss = loss_fn(vectors_a, vectors_b, vectors_b[negative_msk])\n",
        "  return loss"
      ],
      "metadata": {
        "id": "HAevo7Xh9UUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hard_negative_criterion(vectors_a: torch.Tensor, vectors_b: torch.Tensor, labels: torch.Tensor, loss_fn: torch.nn.TripletMarginLoss) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Calculates triplet loss to map representations `vectors_a` and `vectors_b` to the same vectors.\n",
        "  The hard negative sample is chosen based on the dot product between `vectors_a` and `vectors_b`.\n",
        "  The function returns the calculated loss value.\n",
        "\n",
        "  Parameters:\n",
        "  vectors_a (torch.Tensor): A tensor of shape (batch_size, embedding_size) representing the embeddings for the first set of vectors.\n",
        "  vectors_b (torch.Tensor): A tensor of shape (batch_size, embedding_size) representing the embeddings for the second set of vectors.\n",
        "  labels (torch.Tensor): A tensor of shape (batch_size) representing the label / class for each sample in `vectors_a` and `vectors_b`.\n",
        "  loss_fn ([torch.nn.TripletMarginLoss], optional): The triplet margin loss function. Defaults to `loss_fn` defined in the global scope.\n",
        "\n",
        "  Returns:\n",
        "  torch.Tensor: A tensor representing the calculated loss value.\n",
        "  \"\"\"\n",
        "  positive_msk = (labels.unsqueeze(0) == labels.unsqueeze(1))\n",
        "\n",
        "  # dot_product = torch.matmul(vectors_a, vectors_b.t())\n",
        "  cos_sim = cosine_similarity_matrix(vectors_a, vectors_b)\n",
        "  cos_sim = torch.where(positive_msk, float('-inf'), cos_sim)\n",
        "  negative_msk = torch.max(cos_sim, dim=1).indices\n",
        "  loss = loss_fn(vectors_a, vectors_b, vectors_b[negative_msk])\n",
        "  return loss"
      ],
      "metadata": {
        "id": "GcHGWav3dyZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def topk_accuracy(vectors_a: torch.Tensor, vectors_b: torch.Tensor, labels_a: torch.Tensor, labels_b: torch.Tensor, k :int=5) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Calculates the top-k accuracy of the predictions made using `vectors_a` and `vectors_b`.\n",
        "  The predictions are made by comparing the cosine similarity between the vectors in `vectors_a` and `vectors_b`.\n",
        "  The function returns the calculated accuracy.\n",
        "\n",
        "  Parameters:\n",
        "  vectors_a (torch.Tensor): A tensor of shape (batch_size, vector_size).\n",
        "  vectors_b (torch.Tensor): A tensor of shape (batch_size, vector_size).\n",
        "  labels_a (torch.Tensor): A tensor of shape (batch_size) representing the labels for each sample in `vectors_a`.\n",
        "  labels_b (torch.Tensor): A tensor of shape (batch_size) representing the labels for each sample in `vectors_b`.\n",
        "  k (int, optional): # of top predictions to consider. Defaults to 5.\n",
        "\n",
        "  Returns:\n",
        "  torch.Tensor: Accuracy.\n",
        "  \"\"\"\n",
        "  pos_mask = (labels_a.unsqueeze(1) == labels_b.unsqueeze(0))\n",
        "  sim_matrix = cosine_similarity_matrix(vectors_a, vectors_b)\n",
        "  topk_mask = torch.topk(sim_matrix, k=k).indices\n",
        "  topk_pos = pos_mask.gather(dim=1, index=topk_mask)\n",
        "  true_pred = torch.any(topk_pos, dim=1)\n",
        "  return torch.sum(true_pred) / true_pred.nelement()"
      ],
      "metadata": {
        "id": "_BAg6snll6Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Utils"
      ],
      "metadata": {
        "id": "OhxU5frmzpTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # has to be assigned here as some iterators\n",
        "    # change length after each loop\n",
        "    len_iterator = len(iterator)\n",
        "\n",
        "    for (a, b, labels) in iterator:\n",
        "\n",
        "        a = a.to(device)\n",
        "        b = b.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        a_t, b_t = model(a, b)\n",
        "        \n",
        "        loss = criterion(a_t, b_t, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    return epoch_loss / len_iterator"
      ],
      "metadata": {
        "id": "rNtONVt7zoqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    len_iterator = len(iterator)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for (a, b, labels) in iterator:\n",
        "\n",
        "            a = a.to(device)\n",
        "            b = b.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            a_t, b_t = model(a, b)\n",
        "            \n",
        "            loss = criterion(a_t, b_t, labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "    \n",
        "    return epoch_loss / len_iterator"
      ],
      "metadata": {
        "id": "W0jhtggrzthN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=4, verbose=False, delta=1e-3, path='checkpoint.pt', trace_func=print, save_checkpoint_file=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 4\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.val_acc_max = np.NINF\n",
        "        self.time_at_stop = 0\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "        self.save_checkpoint_file = save_checkpoint_file\n",
        "\n",
        "        # TEMP: DEBUG\n",
        "        self.n_calls = 0\n",
        "    def __call__(self, val_loss, val_acc, model, current_time=0):\n",
        "        self.n_calls += 1\n",
        "\n",
        "        if val_loss > self.val_loss_min - self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.save_checkpoint(val_loss, val_acc, model)\n",
        "            self.counter = 0\n",
        "            self.time_at_stop = current_time\n",
        "\n",
        "    def save_checkpoint(self, val_loss, val_acc, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).')\n",
        "        if self.save_checkpoint_file:\n",
        "          torch.save(model.state_dict(), self.path)\n",
        "          print(f'{self.n_calls:3d}. call: Saving model...')\n",
        "        self.val_loss_min = val_loss\n",
        "        self.val_acc_max = val_acc"
      ],
      "metadata": {
        "id": "XPId5jtNz3UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_planned(model, train_dataloader, test_loader, criterion, device='cpu', num_epochs=75, early_stop=True, patience=5, save_model=False, losses=None):\n",
        "  pbar = trange(num_epochs, desc='Training', position=0, leave=True)\n",
        "\n",
        "  early_stopping = EarlyStopping(verbose=True, patience=patience, delta=1e-5, trace_func=pbar.set_description, save_checkpoint_file=save_model)\n",
        "  train_losses, test_losses = [], []\n",
        "\n",
        "  info = {\n",
        "      'train_losses': train_losses,\n",
        "      'test_losses': test_losses,\n",
        "      'loss': np.Inf,\n",
        "      'epochs': num_epochs,\n",
        "      'time': 0\n",
        "  }\n",
        "\n",
        "  for epoch in pbar:\n",
        "\n",
        "    start = time.time()\n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion, device)\n",
        "    stop = time.time()\n",
        "\n",
        "    test_loss = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "    info['loss'] = test_loss\n",
        "    info['time'] = info['time'] + stop - start\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    early_stopping(test_loss, 1, model, current_time=info['time'])\n",
        "\n",
        "    pbar.set_description(f'Test / Train | Loss: {test_loss:.3f}/{train_loss:.3f}')\n",
        "\n",
        "    if early_stop and early_stopping.early_stop:\n",
        "      pbar.close()\n",
        "      print(f'Early stopping. Completed {epoch}/{num_epochs} epochs.')\n",
        "\n",
        "      info['loss'] = early_stopping.val_loss_min\n",
        "      info['time'] = early_stopping.time_at_stop\n",
        "      # Number of epochs the reported model were trained for\n",
        "      info['epochs'] = epoch - early_stopping.patience + 1\n",
        "\n",
        "      return info\n",
        "  \n",
        "  pbar.close()\n",
        "  return info"
      ],
      "metadata": {
        "id": "jhLDjb2pz3t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data"
      ],
      "metadata": {
        "id": "O-0XscFsurWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(filepath):\n",
        "    with open(filepath, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "def print_shapes(arrays):\n",
        "  max_name_length = max(len(name) for name in arrays)\n",
        "  for name, array in arrays.items():\n",
        "    print(f'{name:{max_name_length}}: {array.shape}')"
      ],
      "metadata": {
        "id": "mGkrytb31EpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parent_folder = pathlib.Path('/content/drive/MyDrive/collective_learning/px-multimodal-repr/binaries/flickr8k')\n",
        "\n",
        "train_img_vectors_path = parent_folder / 'train_img_vectors.pkl'\n",
        "train_text_vectors_path = parent_folder / 'train_text_vectors.pkl'\n",
        "dev_img_vectors_path = parent_folder / 'dev_img_vectors.pkl'\n",
        "dev_text_vectors_path = parent_folder / 'dev_text_vectors.pkl'\n",
        "test_img_vectors_path = parent_folder / 'test_img_vectors.pkl'\n",
        "test_text_vectors_path = parent_folder / 'test_text_vectors.pkl'\n",
        "\n",
        "train_img_vectors = load_data(train_img_vectors_path)\n",
        "train_text_vectors = load_data(train_text_vectors_path)\n",
        "dev_img_vectors = load_data(dev_img_vectors_path)\n",
        "dev_text_vectors = load_data(dev_text_vectors_path)\n",
        "test_img_vectors = load_data(test_img_vectors_path)\n",
        "test_text_vectors = load_data(test_text_vectors_path)"
      ],
      "metadata": {
        "id": "oPP22MKAut9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arrays = {\n",
        "    'train_img_vectors': train_img_vectors,\n",
        "    'train_text_vectors': train_text_vectors,\n",
        "    'dev_img_vectors': dev_img_vectors,\n",
        "    'dev_text_vectors': dev_text_vectors,\n",
        "    'test_img_vectors': test_img_vectors,\n",
        "    'test_text_vectors': test_text_vectors,\n",
        "}\n",
        "print_shapes(arrays)"
      ],
      "metadata": {
        "id": "UDbdn48G7yfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each image has 5 captions\n",
        "n_captions = 5\n",
        "\n",
        "all_img_vectors = np.concatenate((train_img_vectors, dev_img_vectors, test_img_vectors))\n",
        "num_images = all_img_vectors.shape[0]\n",
        "all_labels = np.arange(num_images)\n",
        "\n",
        "num_train_images = train_img_vectors.shape[0]\n",
        "train_labels = np.repeat(all_labels[:num_train_images], n_captions)\n",
        "\n",
        "num_dev_images = dev_img_vectors.shape[0]\n",
        "dev_labels = np.repeat(all_labels[num_train_images:num_train_images + num_dev_images], n_captions)\n",
        "\n",
        "num_test_images = test_img_vectors.shape[0]\n",
        "test_labels = np.repeat(all_labels[num_train_images + num_dev_images:], n_captions)\n",
        "\n",
        "train_img_vectors = np.repeat(train_img_vectors, n_captions, axis=0)\n",
        "dev_img_vectors = np.repeat(dev_img_vectors, n_captions, axis=0)\n",
        "test_img_vectors = np.repeat(test_img_vectors, n_captions, axis=0)"
      ],
      "metadata": {
        "id": "1l-oMDPsJoXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_vectors, train_text_vectors = torch.Tensor(train_img_vectors), torch.Tensor(train_text_vectors)\n",
        "dev_img_vectors, dev_text_vectors = torch.Tensor(dev_img_vectors), torch.Tensor(dev_text_vectors)\n",
        "test_img_vectors, test_text_vectors = torch.Tensor(test_img_vectors), torch.Tensor(test_text_vectors)\n",
        "\n",
        "train_labels = torch.Tensor(train_labels)\n",
        "dev_labels = torch.Tensor(dev_labels)\n",
        "test_labels = torch.Tensor(test_labels)"
      ],
      "metadata": {
        "id": "6qUv8hwr7WEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arrays = {\n",
        "    'train_img_vectors': train_img_vectors,\n",
        "    'train_text_vectors': train_text_vectors,\n",
        "    'dev_img_vectors': dev_img_vectors,\n",
        "    'dev_text_vectors': dev_text_vectors,\n",
        "    'test_img_vectors': test_img_vectors,\n",
        "    'test_text_vectors': test_text_vectors,\n",
        "}\n",
        "print_shapes(arrays)"
      ],
      "metadata": {
        "id": "NBjIE_n3QetI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(train_img_vectors, train_text_vectors, train_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "dev_dataset = TensorDataset(dev_img_vectors, dev_text_vectors, dev_labels)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(test_img_vectors, test_text_vectors, test_labels)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "krALCV2A5o3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definitions"
      ],
      "metadata": {
        "id": "ppoPm9o5tVq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageModel(nn.Module):\n",
        "    def __init__(self, img_dim, transformed_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(img_dim, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 256)\n",
        "        self.fc3 = nn.Linear(256, transformed_dim)\n",
        "        self.l2_norm = nn.utils.weight_norm(self.fc3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # x = self.l2_norm(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4rFmAwviqhv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextModel(nn.Module):\n",
        "    def __init__(self, text_dim, transformed_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(text_dim, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, transformed_dim)\n",
        "        self.l2_norm = nn.utils.weight_norm(self.fc3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # x = self.l2_norm(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "sCtEl_vwqmtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MergedModel(nn.Module):\n",
        "    def __init__(self, img_dim, text_dim, transformed_dim):\n",
        "        super().__init__()\n",
        "        self.img_model = ImageModel(img_dim, transformed_dim)\n",
        "        self.text_model = TextModel(text_dim, transformed_dim)\n",
        "\n",
        "    def forward(self, img_vectors, text_vectors):\n",
        "        img_output = self.img_model(img_vectors)\n",
        "        text_output = self.text_model(text_vectors)\n",
        "        return img_output, text_output"
      ],
      "metadata": {
        "id": "Np3I2SD77ge_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MergedModel(img_dim=train_img_vectors.shape[1], text_dim=train_text_vectors.shape[1], transformed_dim=128)\n",
        "loss_fn = torch.nn.TripletMarginLoss(margin=1.0)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)"
      ],
      "metadata": {
        "id": "4gfKIhG6scVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "spx6WIXOqYzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_img_vectors = load_data(dev_img_vectors_path)\n",
        "dev_img_transformed = model.img_model(torch.Tensor(dev_img_vectors))\n",
        "dev_text_transformed = model.text_model(torch.Tensor(dev_text_vectors))"
      ],
      "metadata": {
        "id": "CRVSykJ-YD2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topk_accuracy(dev_img_transformed, dev_text_transformed, torch.arange(num_dev_images), torch.arange(num_dev_images).repeat_interleave(n_captions))"
      ],
      "metadata": {
        "id": "-X6jVUAVU0aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info = train_planned(model, train_dataloader, dev_dataloader, lambda a, b, labels: hard_negative_criterion(a, b, labels, loss_fn), device=device, num_epochs=13, early_stop=True, patience=5)"
      ],
      "metadata": {
        "id": "C15qlqgS742U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_img_vectors = load_data(test_img_vectors_path)\n",
        "test_img_transformed = model.img_model(torch.Tensor(test_img_vectors))\n",
        "test_text_transformed = model.text_model(torch.Tensor(test_text_vectors))\n",
        "topk_accuracy(test_img_transformed, test_text_transformed, torch.arange(num_test_images), torch.arange(num_test_images).repeat_interleave(n_captions), k=5)"
      ],
      "metadata": {
        "id": "bkom4aNjV4jw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}