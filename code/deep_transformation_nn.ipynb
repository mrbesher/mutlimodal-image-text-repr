{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nmgb9wu59VCh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U sentence-transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owLDtqDHaYsH"
      },
      "outputs": [],
      "source": [
        "from os import path\n",
        "\n",
        "from tqdm import trange\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torchvision.io import read_image\n",
        "from datasets import Dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "import torchvision\n",
        "import torch\n",
        "import pickle\n",
        "import pathlib\n",
        "import collections\n",
        "import urllib\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhgVlbSavJXk"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsZz_rsyxVI-"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "N_CAPTIONS = 5\n",
        "TARGET_DIM = 128\n",
        "\n",
        "sentence_transformer_ckp = 'nreimers/MiniLM-L6-H384-uncased'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc2i47dnfjbW"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WtnhF3O06uW"
      },
      "source": [
        "## Generic Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsMo7mQV5PAq"
      },
      "outputs": [],
      "source": [
        "def dict_to_device(d, device):\n",
        "  return {k: v.to(device) for k, v in d.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjr8w0qi0-VY"
      },
      "outputs": [],
      "source": [
        "def repeat_list(my_list, n):\n",
        "  return [x for x in my_list for _ in range(n)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQbpRLiT0sYw"
      },
      "source": [
        "## Selection Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY3zGRRBQQ5U"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity_matrix(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Source: https://stackoverflow.com/a/50426321\n",
        "  \"\"\"\n",
        "  a = a / a.norm(dim=-1, keepdim=True)\n",
        "  b = b / b.norm(dim=-1, keepdim=True)\n",
        "  return a @ b.t()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9R58LEr0qLm"
      },
      "source": [
        "## Metrics and Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAevo7Xh9UUU"
      },
      "outputs": [],
      "source": [
        "def random_negative_criterion(vectors_a: torch.Tensor, vectors_b: torch.Tensor, labels: torch.Tensor, loss_fn: torch.nn.TripletMarginLoss) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Calculates triplet loss to map representations `vectors_a` and `vectors_b` to the same vectors.\n",
        "  The hard negative sample is chosen based on the dot product between `vectors_a` and `vectors_b`.\n",
        "  The function returns the calculated loss value.\n",
        "\n",
        "  Parameters:\n",
        "  vectors_a (torch.Tensor): A tensor of shape (batch_size, embedding_size) representing the embeddings for the first set of vectors.\n",
        "  vectors_b (torch.Tensor): A tensor of shape (batch_size, embedding_size) representing the embeddings for the second set of vectors.\n",
        "  labels (torch.Tensor): A tensor of shape (batch_size) representing the label / class for each sample in `vectors_a` and `vectors_b`.\n",
        "  loss_fn ([torch.nn.TripletMarginLoss], optional): The triplet margin loss function. Defaults to `loss_fn` defined in the global scope.\n",
        "\n",
        "  Returns:\n",
        "  torch.Tensor: A tensor representing the calculated loss value.\n",
        "  \"\"\"\n",
        "  non_positive_msk = (labels.unsqueeze(0) != labels.unsqueeze(1))\n",
        "\n",
        "  # dot_product = torch.matmul(vectors_a, vectors_b.t())\n",
        "  negative_msk = torch.max(torch.rand(non_positive_msk.shape, device=non_positive_msk.get_device()) * non_positive_msk, dim=1).indices\n",
        "  loss = loss_fn(vectors_a, vectors_b, vectors_b[negative_msk])\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcHGWav3dyZa"
      },
      "outputs": [],
      "source": [
        "def hard_negative_criterion(vectors_a: torch.Tensor, vectors_b: torch.Tensor, labels: torch.Tensor, loss_fn: torch.nn.TripletMarginLoss) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Calculates triplet loss to map representations `vectors_a` and `vectors_b` to the same vectors.\n",
        "  The hard negative sample is chosen based on the dot product between `vectors_a` and `vectors_b`.\n",
        "  The function returns the calculated loss value.\n",
        "\n",
        "  Parameters:\n",
        "  vectors_a (torch.Tensor): A tensor of shape (batch_size, embedding_size) representing the embeddings for the first set of vectors.\n",
        "  vectors_b (torch.Tensor): A tensor of shape (batch_size, embedding_size) representing the embeddings for the second set of vectors.\n",
        "  labels (torch.Tensor): A tensor of shape (batch_size) representing the label / class for each sample in `vectors_a` and `vectors_b`.\n",
        "  loss_fn ([torch.nn.TripletMarginLoss], optional): The triplet margin loss function. Defaults to `loss_fn` defined in the global scope.\n",
        "\n",
        "  Returns:\n",
        "  torch.Tensor: A tensor representing the calculated loss value.\n",
        "  \"\"\"\n",
        "  positive_msk = (labels.unsqueeze(0) == labels.unsqueeze(1))\n",
        "\n",
        "  # dot_product = torch.matmul(vectors_a, vectors_b.t())\n",
        "  cos_sim = cosine_similarity_matrix(vectors_a, vectors_b)\n",
        "  cos_sim = torch.where(positive_msk, float('-inf'), cos_sim)\n",
        "  negative_msk = torch.max(cos_sim, dim=1).indices\n",
        "  loss = loss_fn(vectors_a, vectors_b, vectors_b[negative_msk])\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BAg6snll6Ki"
      },
      "outputs": [],
      "source": [
        "def topk_accuracy(vectors_a: torch.Tensor, vectors_b: torch.Tensor, labels_a: torch.Tensor, labels_b: torch.Tensor, k :int=5) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Calculates the top-k accuracy of the predictions made using `vectors_a` and `vectors_b`.\n",
        "  The predictions are made by comparing the cosine similarity between the vectors in `vectors_a` and `vectors_b`.\n",
        "  The function returns the calculated accuracy.\n",
        "\n",
        "  Parameters:\n",
        "  vectors_a (torch.Tensor): A tensor of shape (batch_size, vector_size).\n",
        "  vectors_b (torch.Tensor): A tensor of shape (batch_size, vector_size).\n",
        "  labels_a (torch.Tensor): A tensor of shape (batch_size) representing the labels for each sample in `vectors_a`.\n",
        "  labels_b (torch.Tensor): A tensor of shape (batch_size) representing the labels for each sample in `vectors_b`.\n",
        "  k (int, optional): # of top predictions to consider. Defaults to 5.\n",
        "\n",
        "  Returns:\n",
        "  torch.Tensor: Accuracy.\n",
        "  \"\"\"\n",
        "  pos_mask = (labels_a.unsqueeze(1) == labels_b.unsqueeze(0))\n",
        "  sim_matrix = cosine_similarity_matrix(vectors_a, vectors_b)\n",
        "  topk_mask = torch.topk(sim_matrix, k=k).indices\n",
        "  topk_pos = pos_mask.gather(dim=1, index=topk_mask)\n",
        "  true_pred = torch.any(topk_pos, dim=1)\n",
        "  return torch.sum(true_pred) / true_pred.nelement()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhxU5frmzpTP"
      },
      "source": [
        "# Train Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNtONVt7zoqd"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # has to be assigned here as some iterators\n",
        "    # change length after each loop\n",
        "    len_iterator = len(iterator)\n",
        "\n",
        "    for (a, b, labels) in iterator:\n",
        "\n",
        "        a = a.to(device)\n",
        "        b = dict_to_device(b, device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        a_t, b_t = model(a, b)\n",
        "        \n",
        "        loss = criterion(a_t, b_t, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    return epoch_loss / len_iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0jhtggrzthN"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    len_iterator = len(iterator)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for (a, b, labels) in iterator:\n",
        "\n",
        "            a = a.to(device)\n",
        "            b = dict_to_device(b, device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            a_t, b_t = model(a, b)\n",
        "            \n",
        "            loss = criterion(a_t, b_t, labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "    \n",
        "    return epoch_loss / len_iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPId5jtNz3UC"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=4, verbose=False, delta=1e-3, path='checkpoint.pt', trace_func=print, save_checkpoint_file=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 4\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.val_acc_max = np.NINF\n",
        "        self.time_at_stop = 0\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "        self.save_checkpoint_file = save_checkpoint_file\n",
        "\n",
        "        # TEMP: DEBUG\n",
        "        self.n_calls = 0\n",
        "    def __call__(self, val_loss, val_acc, model, current_time=0):\n",
        "        self.n_calls += 1\n",
        "\n",
        "        if val_loss > self.val_loss_min - self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.save_checkpoint(val_loss, val_acc, model)\n",
        "            self.counter = 0\n",
        "            self.time_at_stop = current_time\n",
        "\n",
        "    def save_checkpoint(self, val_loss, val_acc, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).')\n",
        "        if self.save_checkpoint_file:\n",
        "          torch.save(model.state_dict(), self.path)\n",
        "          print(f'{self.n_calls:3d}. call: Saving model...')\n",
        "        self.val_loss_min = val_loss\n",
        "        self.val_acc_max = val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhLDjb2pz3t2"
      },
      "outputs": [],
      "source": [
        "def train_planned(model, train_dataloader, test_loader, criterion, device='cpu', num_epochs=75, early_stop=True, patience=5, save_model=False, losses=None):\n",
        "  pbar = trange(num_epochs, desc='Training', position=0, leave=True)\n",
        "\n",
        "  early_stopping = EarlyStopping(verbose=True, patience=patience, delta=1e-3, trace_func=pbar.set_description, save_checkpoint_file=save_model)\n",
        "  train_losses, test_losses = [], []\n",
        "\n",
        "  info = {\n",
        "      'train_losses': train_losses,\n",
        "      'test_losses': test_losses,\n",
        "      'loss': np.Inf,\n",
        "      'epochs': num_epochs,\n",
        "      'time': 0\n",
        "  }\n",
        "\n",
        "  for epoch in pbar:\n",
        "\n",
        "    start = time.time()\n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion, device)\n",
        "    stop = time.time()\n",
        "\n",
        "    test_loss = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "    info['loss'] = test_loss\n",
        "    info['time'] = info['time'] + stop - start\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    early_stopping(test_loss, 1, model, current_time=info['time'])\n",
        "\n",
        "    pbar.set_description(f'Test / Train | Loss: {test_loss:.3f}/{train_loss:.3f}')\n",
        "\n",
        "    if early_stop and early_stopping.early_stop:\n",
        "      pbar.close()\n",
        "      print(f'Early stopping. Completed {epoch}/{num_epochs} epochs.')\n",
        "\n",
        "      info['loss'] = early_stopping.val_loss_min\n",
        "      info['time'] = early_stopping.time_at_stop\n",
        "      # Number of epochs the reported model were trained for\n",
        "      info['epochs'] = epoch - early_stopping.patience + 1\n",
        "\n",
        "      return info\n",
        "  \n",
        "  pbar.close()\n",
        "  return info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO5_4sqVt78O"
      },
      "source": [
        "# Model Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HduCoDPoagi3"
      },
      "source": [
        "## Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lh29PxScah8W"
      },
      "outputs": [],
      "source": [
        "sentence_model = SentenceTransformer(sentence_transformer_ckp)\n",
        "tokenized = sentence_model.tokenize(['dummy_text'])\n",
        "text_feature_dim = sentence_model(tokenized)['sentence_embedding'].nelement()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwJ-hDAriCzI"
      },
      "outputs": [],
      "source": [
        "del sentence_model, tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMUYT-cFu9Az"
      },
      "source": [
        "## Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yU2-n8V1t3dd"
      },
      "outputs": [],
      "source": [
        "resnet_weights = torchvision.models.ResNet18_Weights.DEFAULT\n",
        "\n",
        "transforms = resnet_weights.transforms()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKDBtd4RJmug"
      },
      "outputs": [],
      "source": [
        "def init_base_img_model(weights):\n",
        "  image_model = torchvision.models.resnet18(weights)\n",
        "  image_feature_layers = list(image_model.children())[:-2]\n",
        "  return nn.Sequential(*image_feature_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fbEYMqwbpgC"
      },
      "outputs": [],
      "source": [
        "image_model = init_base_img_model(resnet_weights)\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "image_feature_shape = image_model(dummy_input).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9bnR-FHp-XF"
      },
      "outputs": [],
      "source": [
        "del image_model, dummy_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZxDrTcGk30c"
      },
      "source": [
        "## Merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rFmAwviqhv_"
      },
      "outputs": [],
      "source": [
        "class ImageFCModel(nn.Module):\n",
        "  def __init__(self, img_shape, transformed_dim, maxpool_kernel_size=2):\n",
        "    super().__init__()\n",
        "    self.maxpool_kernel_size = maxpool_kernel_size\n",
        "    self.img_base_model = init_base_img_model(resnet_weights)\n",
        "\n",
        "    # calc img dim after feature extraction\n",
        "    img_dim = F.max_pool2d(torch.randn(img_shape), kernel_size=self.maxpool_kernel_size).view(-1).nelement()\n",
        "\n",
        "    self.fc1 = nn.Linear(img_dim, 1024)\n",
        "    self.fc2 = nn.Linear(1024, 256)\n",
        "    self.fc3 = nn.Linear(256, transformed_dim)\n",
        "    self.l2_norm = nn.utils.weight_norm(self.fc3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.img_base_model(x)\n",
        "    x = F.max_pool2d(x, self.maxpool_kernel_size)\n",
        "    x = torch.flatten(x, start_dim=1) # flatten\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    # x = self.l2_norm(x)\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCtEl_vwqmtL"
      },
      "outputs": [],
      "source": [
        "class TextFCModel(nn.Module):\n",
        "  def __init__(self, text_dim, transformed_dim, sentence_transformer_ckp='nreimers/MiniLM-L6-H384-uncased'):\n",
        "    super().__init__()\n",
        "    self.text_transformer = SentenceTransformer(sentence_transformer_ckp, device=device)\n",
        "\n",
        "    self.fc1 = nn.Linear(text_dim, 512)\n",
        "    self.fc2 = nn.Linear(512, 256)\n",
        "    self.fc3 = nn.Linear(256, transformed_dim)\n",
        "    self.l2_norm = nn.utils.weight_norm(self.fc3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    with torch.no_grad():\n",
        "      x = self.text_transformer(x)['sentence_embedding']\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    # x = self.l2_norm(x)\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np3I2SD77ge_"
      },
      "outputs": [],
      "source": [
        "class MergedModel(nn.Module):\n",
        "  def __init__(self, img_shape, text_dim, transformed_dim, sentence_transformer_ckp='nreimers/MiniLM-L6-H384-uncased', maxpool_kernel_size=2):\n",
        "    super().__init__()\n",
        "\n",
        "    self.img_model = ImageFCModel(img_shape, transformed_dim, maxpool_kernel_size=maxpool_kernel_size)\n",
        "    self.text_model = TextFCModel(text_dim, transformed_dim, sentence_transformer_ckp=sentence_transformer_ckp)\n",
        "\n",
        "  def forward(self, img, tokenized_text):\n",
        "    img = self.img_model(img)\n",
        "    text_x = self.text_model(tokenized_text)\n",
        "\n",
        "    return img, text_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TfHM7KGZuu1"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8cmV8OJtb_B"
      },
      "source": [
        "## Download The Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_bXre-gL092"
      },
      "outputs": [],
      "source": [
        "dataset_img_path = pathlib.Path('flickr8k') / 'Flicker8k_Dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCcXQ78Lvh71"
      },
      "outputs": [],
      "source": [
        "# Reference: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/image_captioning.ipynb#scrollTo=kaNy_l7tGuAZ&line=1&uniqifier=1\n",
        "\n",
        "def flickr8k(path='flickr8k'):\n",
        "  path = pathlib.Path(path)\n",
        "  path = pathlib.Path(path)\n",
        "  dataset_path = path / 'Flicker8k_Dataset'\n",
        "\n",
        "  if not dataset_path.exists():\n",
        "    url = 'https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip'\n",
        "    file_path, _ = urllib.request.urlretrieve(url)\n",
        "    zip_ref = zipfile.ZipFile(file_path, 'r')\n",
        "    zip_ref.extractall(path)\n",
        "    zip_ref.close()\n",
        "\n",
        "    url = 'https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip'\n",
        "    file_path, _ = urllib.request.urlretrieve(url)\n",
        "    zip_ref = zipfile.ZipFile(file_path, 'r')\n",
        "    zip_ref.extractall(path)\n",
        "    zip_ref.close()\n",
        "    \n",
        "  captions = (path/\"Flickr8k.token.txt\").read_text().splitlines()\n",
        "  captions = (line.split('\\t') for line in captions)\n",
        "  captions = ((fname.split('#')[0], caption) for (fname, caption) in captions)\n",
        "\n",
        "  cap_dict = collections.defaultdict(list)\n",
        "  for fname, cap in captions:\n",
        "    cap_dict[fname].append(cap)\n",
        "\n",
        "  train_files = (path/'Flickr_8k.trainImages.txt').read_text().splitlines()\n",
        "  train_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in train_files]\n",
        "\n",
        "  dev_files = (path/'Flickr_8k.devImages.txt').read_text().splitlines()\n",
        "  dev_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in dev_files]\n",
        "\n",
        "  test_files = (path/'Flickr_8k.testImages.txt').read_text().splitlines()\n",
        "  test_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in test_files]\n",
        "\n",
        "  return train_captions, dev_captions, test_captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-lZhY-tvpbD"
      },
      "outputs": [],
      "source": [
        "train_raw, dev_raw, test_raw = flickr8k()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYG204ujtgWo"
      },
      "source": [
        "## Preprocess The Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XEutOOHNHVf"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "  def __init__(self, image_paths, transform=None, target_transform=None):\n",
        "    self.image_paths = image_paths\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_paths)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = self.image_paths[idx]\n",
        "    image = read_image(str(img_path))\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mG_aO3s73uE5"
      },
      "outputs": [],
      "source": [
        "class MergedDataset(Dataset):\n",
        "  def __init__(self, datasets):\n",
        "    self.datasets = datasets\n",
        "\n",
        "  def __len__(self):\n",
        "    return min(len(ds) for ds in self.datasets)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return tuple(ds[idx] for ds in self.datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q3u-SFjurUY"
      },
      "source": [
        "## Create Text Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkapWqRc4fpv"
      },
      "outputs": [],
      "source": [
        "train_captions = [cap for _, captions in train_raw for cap in captions]\n",
        "dev_captions = [cap for _, captions in dev_raw for cap in captions]\n",
        "test_captions = [cap for _, captions in test_raw for cap in captions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXTDG3c1uvEw"
      },
      "outputs": [],
      "source": [
        "sentence_transformer = SentenceTransformer(sentence_transformer_ckp, device='cpu')\n",
        "\n",
        "tokenized_train_text = sentence_transformer.tokenize(train_captions)\n",
        "tokenized_dev_text = sentence_transformer.tokenize(dev_captions)\n",
        "tokenized_test_text = sentence_transformer.tokenize(test_captions)\n",
        "\n",
        "del sentence_transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mbz3RTZVwlMH"
      },
      "outputs": [],
      "source": [
        "text_train_ds = Dataset.from_dict(tokenized_train_text)\n",
        "text_dev_ds = Dataset.from_dict(tokenized_dev_text)\n",
        "text_test_ds = Dataset.from_dict(tokenized_test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEsAV-gtPcLH"
      },
      "outputs": [],
      "source": [
        "text_train_ds.set_format('torch')\n",
        "text_dev_ds.set_format('torch')\n",
        "text_test_ds.set_format('torch')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgzb2R5Dus6O"
      },
      "source": [
        "## Create Image Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ikut8YSGMLj3"
      },
      "outputs": [],
      "source": [
        "img_train_paths = [path for path, _ in train_raw]\n",
        "img_dev_paths = [path for path, _ in dev_raw]\n",
        "img_test_paths = [path for path, _ in test_raw]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stYO0Ak8Sa2r"
      },
      "outputs": [],
      "source": [
        "start = 0\n",
        "end = len(img_train_paths)\n",
        "label_train_tensor = torch.arange(start=start, end=end)\n",
        "\n",
        "start = end\n",
        "end += len(img_dev_paths)\n",
        "label_dev_tensor = torch.arange(start=start, end=end)\n",
        "\n",
        "start = end\n",
        "end += len(img_test_paths)\n",
        "label_test_tensor = torch.arange(start=start, end=end)\n",
        "\n",
        "# repeat the label for the number of captions. each image is repeated 5 times, \n",
        "# so the other captions have the same label as the image\n",
        "label_train_tensor = torch.repeat_interleave(label_train_tensor, repeats=N_CAPTIONS)\n",
        "label_dev_tensor = torch.repeat_interleave(label_dev_tensor, repeats=N_CAPTIONS)\n",
        "label_test_tensor = torch.repeat_interleave(label_test_tensor, repeats=N_CAPTIONS)\n",
        "\n",
        "label_train_ds = label_train_tensor\n",
        "label_dev_ds = label_dev_tensor\n",
        "label_test_ds = label_test_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3AFGMqm1NKB"
      },
      "outputs": [],
      "source": [
        "# More efficient implementation can be considered\n",
        "# (i.e. the pipeline can be changed to avoid repititon in this step)\n",
        "img_train_paths = repeat_list(img_train_paths, N_CAPTIONS)\n",
        "img_dev_paths = repeat_list(img_dev_paths, N_CAPTIONS)\n",
        "img_test_paths = repeat_list(img_test_paths, N_CAPTIONS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HB3Fup4gOLtv"
      },
      "outputs": [],
      "source": [
        "img_train_ds = CustomImageDataset(img_train_paths, transform=transforms)\n",
        "img_dev_ds = CustomImageDataset(img_dev_paths, transform=transforms)\n",
        "img_test_ds = CustomImageDataset(img_test_paths, transform=transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz-RS8pw4HNx"
      },
      "source": [
        "## Create Merged Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTsbR7NN4JYo"
      },
      "outputs": [],
      "source": [
        "train_ds = MergedDataset([img_train_ds, text_train_ds, label_train_ds])\n",
        "dev_ds = MergedDataset([img_dev_ds, text_dev_ds, label_dev_ds])\n",
        "test_ds = MergedDataset([img_test_ds, text_test_ds, label_test_ds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VrHSt-zO1zH"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRDXfROpofCK"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8OKGz1voikj"
      },
      "outputs": [],
      "source": [
        "model = MergedModel(image_feature_shape, text_feature_dim, TARGET_DIM, sentence_transformer_ckp)\n",
        "loss_fn = torch.nn.TripletMarginLoss(margin=1.0)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3vRc9eYLWhk"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krmUM-36uiMg"
      },
      "outputs": [],
      "source": [
        "info = train_planned(model, train_dataloader, dev_dataloader, lambda a, b, labels: hard_negative_criterion(a, b, labels, loss_fn), device=device, num_epochs=13, early_stop=False, patience=5, save_model=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJcRpLJQOKle"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'model.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}